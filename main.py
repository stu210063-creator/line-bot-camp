import os
import sys
import requests
import urllib.parse
import random
from bs4 import BeautifulSoup
from flask import Flask, request, abort
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import (
    MessageEvent, TextMessage, TextSendMessage, FlexSendMessage
)

# ==========================================
# 1. å…¨åŸŸè¨­å®š
# ==========================================
app = Flask(__name__)

# å¾ç’°å¢ƒè®Šæ•¸è®€å– Token
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get('ruNCmLnmh/ngHJfv4ZtPdATOISHG9kA4hoUkjlrr2+k1wftKHZp9ol7Oirr2L60gLSDEAT1vtJwCphJVYB0v4R2KtYQNChgNAiqb6N4TGVxUYphajdcWmiiY4WcHsj7kFSECb5hSKRAskZTWk+WodAdB04t89/1O/w1cDnyilFU=')
LINE_CHANNEL_SECRET = os.environ.get('7b10d2873a104f96431c43cfd66d0bc2')

if LINE_CHANNEL_ACCESS_TOKEN is None or LINE_CHANNEL_SECRET is None:
    print("âš ï¸ è­¦å‘Šï¼šæœªè¨­å®š LINE Token")

line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(LINE_CHANNEL_SECRET)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# å…¨åŸŸè³‡æ–™åº«
CAMP_DATABASE = []

# ==========================================
# 2. åœ‹é«˜ä¸­ç‡ŸéšŠçˆ¬èŸ² (KKTIX + BeClass)
# ==========================================
class HighSchoolCampScraper:
    def __init__(self):
        self.data_list = []

    def fetch_all(self):
        print("ğŸš€ é–‹å§‹æœå°‹åœ‹é«˜ä¸­ç‡ŸéšŠ...")
        self.data_list = [] 
        
        # 1. æœå°‹ KKTIX (é–å®šã€Œå¤§å­¸ç‡ŸéšŠã€å› ç‚ºé€™æ˜¯çµ¦é«˜ä¸­ç”ŸåƒåŠ çš„)
        self.scrape_kktix(keyword="å¤§å­¸ç‡ŸéšŠ")
        self.scrape_kktix(keyword="é«˜ä¸­é«”é©—ç‡Ÿ")
        
        # 2. æœå°‹ BeClass (è¨±å¤šå­¸è¡“è¬›åº§ã€å¿—å·¥ç‡ŸéšŠ)
        self.scrape_beclass(keyword="é«˜ä¸­ç‡ŸéšŠ")
        
        # å»é™¤é‡è¤‡ (å› ç‚ºä¸åŒé—œéµå­—å¯èƒ½æ‰¾åˆ°åŒä¸€å€‹æ´»å‹•)
        unique_data = []
        seen_urls = set()
        for item in self.data_list:
            if item['url'] not in seen_urls:
                unique_data.append(item)
                seen_urls.add(item['url'])
        
        # éš¨æ©Ÿæ‰“äº‚ï¼Œè®“çµæœçœ‹èµ·ä¾†æ¯”è¼ƒè±å¯Œ
        random.shuffle(unique_data)
        self.data_list = unique_data
        print(f"âœ… çˆ¬èŸ²çµæŸï¼Œå…±æ‰¾åˆ° {len(self.data_list)} ç­†é©åˆåœ‹é«˜ä¸­çš„æ´»å‹•")
        return self.data_list

    def scrape_kktix(self, keyword):
        """KKTIX çˆ¬èŸ²"""
        print(f"æ­£åœ¨ KKTIX æœå°‹: {keyword} ...")
        # KKTIX çš„æœå°‹ç¶²å€çµæ§‹
        url = f"https://kktix.com/events?search={keyword}&start_at=2024-01-01&end_at=2026-12-31"
        
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            events = soup.select('ul.event-list > li')
            
            for event in events:
                try:
                    title_tag = event.select_one('h2')
                    if not title_tag: continue
                    title = title_tag.get_text(strip=True)
                    
                    link_tag = event.select_one('a')
                    link = link_tag['href']
                    if not link.startswith('http'): link = "https://kktix.com" + link
                        
                    time_tag = event.select_one('.date')
                    date_str = time_tag.get_text(strip=True) if time_tag else "è¿‘æœŸæ´»å‹•"
                    
                    # åœ–ç‰‡è™•ç†
                    img_tag = event.select_one('img')
                    image = img_tag['src'] if img_tag else ""
                    if not image: image = "https://images.unsplash.com/photo-1523580494863-6f3031224c94?auto=format&fit=crop&w=600&q=80"

                    self.data_list.append({
                        "title": title,
                        "date": date_str,
                        "source": "KKTIX",
                        "price": "è©³è¦‹ç°¡ç« ",
                        "url": link,
                        "image": image
                    })
                except: continue
        except Exception as e:
            print(f"KKTIX éŒ¯èª¤: {e}")

    def scrape_beclass(self, keyword):
        """BeClass çˆ¬èŸ² (éœ€è¦ç‰¹æ®Šè™•ç†ç·¨ç¢¼)"""
        print(f"æ­£åœ¨ BeClass æœå°‹: {keyword} ...")
        # BeClass çš„æœå°‹ç¶²å€
        encoded_keyword = urllib.parse.quote(keyword)
        url = f"https://www.beclass.com/p/search.php?keyword={encoded_keyword}"
        
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = 'utf-8' # BeClass ä¸»è¦æ˜¯ UTF-8ï¼Œä½†æœ‰æ™‚å€™æœƒäº‚ç¢¼ï¼Œå¼·åˆ¶è¨­å®š
            
            soup = BeautifulSoup(response.text, 'html.parser')
            # BeClass çš„åˆ—è¡¨é€šå¸¸åœ¨ div.search_result_item æˆ–æ˜¯ç›´æ¥æ˜¯é€£çµåˆ—è¡¨
            # é€™è£¡ä½¿ç”¨æ¯”è¼ƒé€šç”¨çš„æŠ“æ³•
            links = soup.find_all('a', href=True)
            
            count = 0
            for link in links:
                href = link['href']
                text = link.get_text(strip=True)
                
                # éæ¿¾æ¢ä»¶ï¼šé€£çµå¿…é ˆåŒ…å« rid (å ±åID) ä¸”æ¨™é¡Œå¤ é•·
                if 'rid=' in href and len(text) > 5 and 'ç‡Ÿ' in text:
                    if not href.startswith('http'):
                        href = "https://www.beclass.com/" + href.lstrip('/')
                    
                    self.data_list.append({
                        "title": text,
                        "date": "è©³è¦‹å…§æ–‡",
                        "source": "BeClass",
                        "price": "å ±åç³»çµ±",
                        "url": href,
                        "image": "https://images.unsplash.com/photo-1517486808906-6ca8b3f04846?auto=format&fit=crop&w=600&q=80" # BeClass å¾ˆé›£æŠ“åœ–ï¼Œçµ±ä¸€ç”¨é è¨­åœ–
                    })
                    count += 1
                    if count >= 8: break # BeClass é›œè¨Šå¤šï¼ŒæŠ“å‰ 8 ç­†å°±å¥½
        except Exception as e:
            print(f"BeClass éŒ¯èª¤: {e}")

# ==========================================
# 3. LINE Flex Message
# ==========================================
def create_camp_flex_message(camps):
    bubbles = []
    for camp in camps[:10]:
        # æ ¹æ“šä¾†æºè¨­å®šä¸åŒé¡è‰²
        source_color = "#E64A19" if camp['source'] == "KKTIX" else "#1976D2"
        
        bubble = {
            "type": "bubble",
            "hero": {
                "type": "image", "url": camp['image'], "size": "full",
                "aspectRatio": "20:13", "aspectMode": "cover",
                "action": { "type": "uri", "uri": camp['url'] }
            },
            "body": {
                "type": "box", "layout": "vertical",
                "contents": [
                    { "type": "text", "text": camp['source'], "size": "xs", "color": source_color, "weight": "bold" },
                    { "type": "text", "text": camp['title'], "weight": "bold", "size": "md", "wrap": True, "margin": "xs" },
                    {
                        "type": "box", "layout": "baseline", "margin": "md",
                        "contents": [
                            { "type": "text", "text": camp['date'], "size": "xs", "color": "#999999", "flex": 1 }
                        ]
                    }
                ]
            },
            "footer": {
                "type": "box", "layout": "vertical", "spacing": "sm",
                "contents": [
                    { "type": "button", "style": "primary", "height": "sm", "action": { "type": "uri", "label": "æŸ¥çœ‹è©³æƒ…", "uri": camp['url'] } }
                ]
            }
        }
        bubbles.append(bubble)
    
    return FlexSendMessage(alt_text="åœ‹é«˜ä¸­ç‡ŸéšŠè³‡è¨Š", contents={"type": "carousel", "contents": bubbles}) if bubbles else None

# ==========================================
# 4. Flask Server
# ==========================================
@app.route("/", methods=['GET'])
def home():
    return "High School Camp Bot is Running!"

@app.route("/callback", methods=['POST'])
def callback():
    signature = request.headers['X-Line-Signature']
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    msg = event.message.text.strip()
    global CAMP_DATABASE
    
    # åªè¦è³‡æ–™åº«æ˜¯ç©ºçš„ï¼Œæˆ–æ˜¯ä½¿ç”¨è€…è¼¸å…¥ç‰¹å®šæŒ‡ä»¤ï¼Œå°±è§¸ç™¼çˆ¬èŸ²
    if not CAMP_DATABASE or msg in ["æ›´æ–°", "ç‡ŸéšŠ", "å¯’å‡", "é«˜ä¸­"]:
        scraper = HighSchoolCampScraper()
        CAMP_DATABASE = scraper.fetch_all()
        
        if msg == "æ›´æ–°":
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text=f"è³‡æ–™æ›´æ–°å®Œç•¢ï¼å…± {len(CAMP_DATABASE)} ç­†åœ‹é«˜ä¸­ç‡ŸéšŠã€‚"))
            return

    # æœå°‹éæ¿¾
    found_camps = []
    keywords = msg.split()
    
    # å¦‚æœä½¿ç”¨è€…è¼¸å…¥å¾ˆç± çµ±çš„è©ï¼Œå›å‚³å…¨éƒ¨
    if msg in ["ç‡ŸéšŠ", "å¯’å‡", "é«˜ä¸­", "æ¨è–¦"]:
        found_camps = CAMP_DATABASE
    else:
        for camp in CAMP_DATABASE:
            if any(k in camp['title'] for k in keywords):
                found_camps.append(camp)

    if found_camps:
        line_bot_api.reply_message(event.reply_token, create_camp_flex_message(found_camps))
    else:
        # æ‰¾ä¸åˆ°æ™‚ï¼Œé™¤äº†
